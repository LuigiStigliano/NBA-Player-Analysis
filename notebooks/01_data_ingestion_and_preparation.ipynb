{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ingestione e Preparazione dei Dati\n",
    "\n",
    "**Obiettivo:** Caricare il dataset grezzo dei giocatori NBA, applicare le funzioni di pulizia e prepararlo per le analisi successive. \n",
    "\n",
    "**Fasi:**\n",
    "1.  **Setup dell'Ambiente:** Configurazione di PySpark e importazione delle librerie.\n",
    "2.  **Caricamento Dati:** Download automatico del dataset da Kaggle (se non presente) e caricamento in un DataFrame Spark.\n",
    "3.  **Pulizia e Standardizzazione:** Applicazione di funzioni per standardizzare i nomi delle colonne, correggere i tipi di dato e gestire i valori mancanti.\n",
    "4.  **Salvataggio:** Salvataggio del DataFrame pulito in formato Parquet per efficienza e interoperabilità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Aggiunge la root del progetto al sys.path per permettere l'importazione dei moduli custom\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Importazione delle utility e delle funzioni di elaborazione\n",
    "from src.utils.helpers import get_spark_session, save_dataframe\n",
    "from src.data_ingestion.download_data import download_nba_dataset\n",
    "from src.data_processing.cleaning import standardize_column_names, correct_data_types, handle_missing_values\n",
    "from src.config.spark_config import SPARK_CONFIG\n",
    "\n",
    "# Inizializzazione della sessione Spark\n",
    "spark = get_spark_session(\n",
    "    app_name=\"NBA_Data_Ingestion_Preparation\",\n",
    "    driver_memory=SPARK_CONFIG[\"driver_memory\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 1: Caricamento Dati con Download Automatico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dei percorsi per i dati grezzi\n",
    "raw_data_dir = \"../data/raw\"\n",
    "csv_file_name = \"Player Totals.csv\"\n",
    "csv_file_path = os.path.join(raw_data_dir, csv_file_name)\n",
    "\n",
    "# Verifica se il dataset esiste, altrimenti lo scarica da Kaggle\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Dataset non trovato. Avvio del download in '{raw_data_dir}'...\")\n",
    "    download_nba_dataset(raw_data_dir)\n",
    "else:\n",
    "    print(f\"Dataset già presente in '{csv_file_path}'. Salto il download.\")\n",
    "\n",
    "# Caricamento del file CSV in un DataFrame Spark\n",
    "raw_df = spark.read.csv(csv_file_path, header=True, inferSchema=False)\n",
    "\n",
    "# Visualizzazione delle informazioni preliminari del DataFrame\n",
    "print(f\"\\nNumero di righe iniziali: {raw_df.count()}\")\n",
    "print(\"Schema iniziale del DataFrame:\")\n",
    "raw_df.printSchema()\n",
    "raw_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 2: Pulizia e Preparazione dei Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicazione sequenziale delle funzioni di pulizia\n",
    "df_std_names = standardize_column_names(raw_df)\n",
    "df_typed = correct_data_types(df_std_names)\n",
    "df_cleaned = handle_missing_values(\n",
    "    df_typed, \n",
    "    min_games_threshold=SPARK_CONFIG[\"min_games_threshold\"]\n",
    ")\n",
    "\n",
    "# Conteggio delle righe dopo il processo di pulizia e filtraggio\n",
    "print(f\"Numero di righe dopo la pulizia: {df_cleaned.count()}\")\n",
    "\n",
    "# Visualizzazione di un campione di dati puliti per verifica\n",
    "df_cleaned.select(\"player\", \"season\", \"pts\", \"mp\", \"g\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 3: Salvataggio dei Dati Processati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione del percorso di output per il file Parquet\n",
    "output_path = \"../data/processed/players_cleaned.parquet\"\n",
    "\n",
    "# Salvataggio del DataFrame pulito usando la funzione helper\n",
    "# Il formato Parquet è scelto per le sue performance e l'ottimizzazione con Spark\n",
    "save_dataframe(df_cleaned, output_path)\n",
    "\n",
    "print(f\"DataFrame pulito salvato con successo in '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Termina la sessione Spark per liberare le risorse\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
