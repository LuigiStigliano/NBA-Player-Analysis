{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ingestione e Preparazione dei Dati\n",
    "\n",
    "**Obiettivo:** Caricare i dati grezzi, pulirli e prepararli per le analisi successive. Questa è la base di tutto il progetto, quindi è fondamentale farla bene.\n",
    "\n",
    "1.  **Setup dell'Ambiente:** Configuro PySpark e importo le librerie che mi servono.\n",
    "2.  **Caricamento Dati:** Scarico automaticamente il dataset da Kaggle e lo carico in uno DataFrame Spark.\n",
    "3.  **Pulizia e Standardizzazione:** Applico le funzioni per standardizzare i nomi delle colonne, correggere i tipi di dato e gestire i valori mancanti.\n",
    "4.  **Salvataggio:** Salvo il DataFrame pulito in formato Parquet, che è ottimo per lavorare con Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.config.spark_config import SPARK_CONFIG\n",
    "from src.data_ingestion.download_data import download_nba_dataset\n",
    "from src.data_processing.cleaning import (correct_data_types, \n",
    "                                          handle_missing_values, \n",
    "                                          standardize_column_names)\n",
    "from src.utils.helpers import get_spark_session, save_dataframe\n",
    "\n",
    "project_root = os.path.abspath(os.path.join('..'))\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "csv_file_path = os.path.join(raw_data_dir, \"Player Totals.csv\")\n",
    "output_path = os.path.join(processed_data_dir, \"players_cleaned.parquet\")\n",
    "\n",
    "spark = get_spark_session(\n",
    "    app_name=\"NBA_Data_Ingestion_Preparation\",\n",
    "    driver_memory=SPARK_CONFIG[\"driver_memory\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 1: Caricamento Dati con Download Automatico\n",
    "\n",
    "Adesso, controllo se ho già il dataset. Se non c'è, lo scarico da Kaggle. Poi lo carico in Spark e do una prima occhiata per capire com'è fatto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Dataset non trovato. Lo scarico in '{raw_data_dir}'...\")\n",
    "    download_nba_dataset(raw_data_dir)\n",
    "else:\n",
    "    print(f\"Dataset già presente in '{csv_file_path}'. Salto il download.\")\n",
    "\n",
    "raw_df = spark.read.csv(csv_file_path, header=True, inferSchema=False)\n",
    "\n",
    "print(f\"\\nNumero di righe iniziali: {raw_df.count()}\")\n",
    "print(\"Schema iniziale del DataFrame:\")\n",
    "raw_df.printSchema()\n",
    "raw_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretazione dell'output\n",
    "\n",
    "Ok, vedo che ho circa 27.000 righe. Come mi aspettavo, tutte le colonne sono state caricate come `string`. Dovrò sistemarle.\n",
    "\n",
    "La tabella di esempio mi conferma che ci sono nomi di colonna strani (es. `3P%`) e valori `NA` da gestire. La pulizia sarà necessaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 2: Pulizia e Preparazione dei Dati\n",
    "\n",
    "Questa è la fase più importante di pre-processing. Applicherò le funzioni in sequenza:\n",
    "1.  **`standardize_column_names`**: Rendo i nomi delle colonne puliti e facili da usare.\n",
    "2.  **`correct_data_types`**: Converto le colonne numeriche da `string` a `double`.\n",
    "3.  **`handle_missing_values`**: Filtro i dati per tenere solo le stagioni dall'era del tiro da tre e i giocatori con un numero minimo di partite giocate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_names = standardize_column_names(raw_df)\n",
    "df_typed = correct_data_types(df_std_names)\n",
    "df_cleaned = handle_missing_values(\n",
    "    df_typed, \n",
    "    min_games_threshold=SPARK_CONFIG[\"min_games_threshold\"]\n",
    ")\n",
    "\n",
    "print(f\"Numero di righe dopo la pulizia: {df_cleaned.count()}\")\n",
    "\n",
    "df_cleaned.select(\"player\", \"season\", \"pts\", \"mp\", \"g\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretazione dell'output\n",
    "\n",
    "Il numero di righe si è ridotto parecchio. Questo è un buon segno, significa che ho tolto i dati che non erano rilevanti per la mia analisi (stagioni troppo vecchie, giocatori con poche partite).\n",
    "\n",
    "Ora le colonne `pts`, `mp` e `g` sono numeriche e pronte per i calcoli. I dati sembrano molto più robusti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 3: Salvataggio dei Dati Processati\n",
    "\n",
    "Ora che il DataFrame è pulito, lo salvo in formato Parquet. Così sarà veloce da caricare nei prossimi notebook e manterrà i tipi di dato corretti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df_cleaned, output_path)\n",
    "\n",
    "print(f\"DataFrame pulito salvato con successo in '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusione e Prossimi Passi\n",
    "\n",
    "Ho completato con successo la prima fase: ho caricato, pulito e preparato i dati. Il risultato è il file `players_cleaned.parquet`, che sarà il punto di partenza per la prossima fase.\n",
    "\n",
    "Nel prossimo notebook, `02_exploratory_data_analysis.ipynb`, userò questo dataset pulito per fare un'analisi esplorativa e cercare le prime intuizioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
